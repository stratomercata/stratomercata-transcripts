# Audio Transcription Pipeline with Multi-Provider AI Processing

Automated transcription with speaker identification using multiple transcription services (WhisperX local, Deepgram, AssemblyAI, OpenAI) and AI post-processing (Claude, GPT-4, Gemini, etc.) to correct technical terms and speaker names.

## Quick Start

```bash
# 1. Setup (one time)
./scripts/install_packages_and_venv.sh
cp setup_env.sh.example setup_env.sh
nano setup_env.sh  # Add your API keys

# 2. Process a single file
source venv/bin/activate && source setup_env.sh
./scripts/process_single.sh audio.mp3 \
  --transcribers whisperx \
  --processors openai

# 3. Process all MP3s in ~/Downloads
./scripts/process_all.sh --transcribers deepgram --processors anthropic
```

## Architecture

```
audio.mp3
    ↓
process_single.sh (orchestration)
    ↓
Phase 1: Transcription
    process_single_transcribe_and_diarize.py
    - Runs all transcribers internally (whisperx, deepgram, assemblyai, openai)
    - Outputs: intermediates/*_transcript_with_speakers.txt
    ↓
Phase 2: Post-Processing  
    process_single_post_process.py
    - Runs all processors internally (anthropic, openai, gemini, etc.)
    - Outputs: outputs/*_corrected.txt
```

## Transcription Services

| Service | Type | Cost/hour | Speed | Diarization |
|---------|------|-----------|-------|-------------|
| **WhisperX** | Local GPU | FREE | 5-10 min | ✅ Yes |
| **Deepgram** | Cloud API | $0.41 | 23 sec | ✅ Yes |
| **AssemblyAI** | Cloud API | $1.44 | 3-4 min | ✅ Yes |
| **OpenAI** | Cloud API | $5.75 | 4-5 min | ❌ No |

## AI Post-Processors

| Provider | Model | Cost | Features |
|----------|-------|------|----------|
| **Anthropic** | Claude 3.5 Sonnet | $3/M tokens | Streaming, 64K output |
| **OpenAI** | GPT-4o | $2.50/M tokens | Chunking, reliable |
| **Gemini** | Gemini 2.5 Pro | $1.25/M tokens | 2M context |
| **DeepSeek** | DeepSeek Chat | $0.14/M tokens | Cost-effective |
| **Moonshot** | Kimi K2 | $0.42/M tokens | 256K context |
| **Ollama** | Qwen2.5:32b | FREE | Local, private |

## Usage Examples

### Single File with One Transcriber/Processor

```bash
# WhisperX (local) + Claude
./scripts/process_single.sh interview.mp3 \
  --transcribers whisperx \
  --processors anthropic

# Deepgram (cloud) + GPT-4
./scripts/process_single.sh interview.mp3 \
  --transcribers deepgram \
  --processors openai
```

### Multiple Transcribers and Processors

```bash
# 2 transcribers × 3 processors = 6 combinations
./scripts/process_single.sh interview.mp3 \
  --transcribers whisperx,deepgram \
  --processors anthropic,openai,ollama
```

### Batch Processing

```bash
# Process all MP3s in ~/Downloads
./scripts/process_all.sh --transcribers deepgram --processors openai

# Use defaults (whisperx + openai)
./scripts/process_all.sh
```

## Setup

### Requirements

**Minimum (CPU):**
- 8GB RAM, 50GB disk
- Expect hours of processing

**Recommended (GPU):**
- NVIDIA GPU with 6GB+ VRAM (GTX 1660+, RTX 20/30/40/50 series)
- 16GB RAM, 50GB disk
- Process 1 hour audio in 5-10 minutes

### Installation

```bash
# Auto-detects hardware and installs dependencies
./scripts/install_packages_and_venv.sh

# For CPU-only on GPU system
./scripts/install_packages_and_venv.sh --force-cpu

# Configure API keys
cp setup_env.sh.example setup_env.sh
nano setup_env.sh  # Add your keys
```

### API Keys

Add to `setup_env.sh`:

```bash
# Required for WhisperX (speaker diarization)
export HF_TOKEN="hf_..."  # https://huggingface.co/settings/tokens

# Optional: Cloud transcription services
export DEEPGRAM_API_KEY="..."      # https://console.deepgram.com/
export ASSEMBLYAI_API_KEY="..."    # https://www.assemblyai.com/
export OPENAI_API_KEY="sk-..."     # https://platform.openai.com/

# Optional: AI post-processors
export ANTHROPIC_API_KEY="sk-ant-..."  # https://console.anthropic.com/
export GOOGLE_API_KEY="..."            # https://makersuite.google.com/
export DEEPSEEK_API_KEY="sk-..."      # https://platform.deepseek.com/
export MOONSHOT_API_KEY="sk-..."      # https://platform.moonshot.cn/
```

## Output Files

### Intermediates (Phase 1)
```
intermediates/
  audio_whisperx_transcript_with_speakers.txt    # WhisperX output
  audio_deepgram_transcript_with_speakers.txt    # Deepgram output
  audio_whisperx_transcript.md                   # Markdown version
```

### Final Outputs (Phase 2)
```
outputs/
  audio_whisperx_anthropic_corrected.txt         # WhisperX + Claude
  audio_deepgram_openai_corrected.txt            # Deepgram + GPT-4
  audio_whisperx_anthropic_corrected.md          # Markdown versions
```

Naming: `{audio}_{transcriber}_{processor}_corrected.{txt|md}`

## GPU Support

### NVIDIA GPUs
- **RTX 50-series** (Blackwell): PyTorch 2.9+ with CUDA 12.8
- **RTX 40-series** (Ada): Full support
- **RTX 30-series** (Ampere): Full support  
- **GTX 16-series** (Turing): Full support

Setup installs appropriate PyTorch version automatically.

### AMD GPUs
Not supported for GPU acceleration. Systems with AMD GPUs run in CPU-only mode.

## Project Structure

```
stratomercata-transcripts/
├── README.md                                    # This file
├── setup_env.sh.example                         # API key template
├── requirements-*.txt                           # Dependencies
├── scripts/
│   ├── install_packages_and_venv.sh            # Setup script
│   ├── process_single.sh                       # Main: single file
│   ├── process_all.sh                          # Main: batch files
│   ├── process_single_transcribe_and_diarize.py  # Phase 1: Transcription
│   ├── process_single_post_process.py          # Phase 2: AI correction
│   ├── extract_people.py                       # Extract speaker names
│   └── extract_terms.py                        # Extract technical terms
├── intermediates/                               # Phase 1 outputs
└── outputs/                                     # Phase 2 outputs
```

## Resources

- [WhisperX](https://github.com/m-bain/whisperX) - GPU-accelerated Whisper with diarization
- [Deepgram](https://developers.deepgram.com/) - Cloud speech-to-text API
- [AssemblyAI](https://www.assemblyai.com/docs) - Cloud transcription API
- [Anthropic Claude](https://docs.anthropic.com/) - AI post-processing
- [OpenAI GPT-4](https://platform.openai.com/docs) - AI post-processing

## License

Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)

**Dependencies:**
- **Core ML**: WhisperX (MIT), PyTorch (BSD-3-Clause), pyannote.audio (MIT)
- **Transcription**: faster-whisper (MIT), deepgram-sdk (MIT), assemblyai (MIT), openai (Apache-2.0)
- **AI Processing**: anthropic (MIT), google-generativeai (Apache-2.0), requests (Apache-2.0)
- **Audio**: ffmpeg (LGPL/GPL), torchaudio (BSD-3-Clause), pandas (BSD-3-Clause)
- **Model Hosting**: speechbrain (Apache-2.0), transformers (Apache-2.0), numpy (BSD-3-Clause)
